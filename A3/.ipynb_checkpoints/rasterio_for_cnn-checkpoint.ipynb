{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a85576",
   "metadata": {},
   "source": [
    "# Preprocessing `.tif` Imagery for CNN Inference using Rasterio\n",
    "\n",
    "Satellite images typically come in `.tif` format, which stores geospatial raster data that is not directly usable by deep learning models like those from TorchGeo. Models such as DOFA require input as PyTorch tensors, normalized and resized to specific dimensions (e.g., 224x224). My specific task in the final project is to integrate pretrained CNNs (DOFA, CopernicusFM) for inference on user-supplied `.tif` data. This notebook demonstrates how to preprocess `.tif` files into tensors ready for model input.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928c47d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Load .tif satellite images using rasterio\n",
    "- Extract RGB bands and convert them into a NumPy array\n",
    "- Preprocess and normalize the data using torchvision.transforms\n",
    "- Convert the processed image into a PyTorch tensor\n",
    "- Run the tensor through a pretrained TorchGeo CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9bfdc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Library\n",
    "import torch\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchgeo.models import DOFA\n",
    "import os\n",
    "from typing import Tuple, List, Optional, Union\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014a73f0-7a4e-4591-99d1-c6929acc8723",
   "metadata": {},
   "source": [
    "### 1. Load `.tif` as Tensor Function\n",
    "\n",
    "This function serves as a converter that transforms a `.tif` image file into a PyTorch tensor, making it compatible as input for a CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "945fce48-f984-4ba6-b095-f97cae27f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tif_as_tensor(\n",
    "    path: str, \n",
    "    size: Tuple[int, int] = (224, 224),\n",
    "    bands: List[int] = [1, 2, 3],  # RGB bands by default\n",
    "    band_order: str = 'RGB',  # 'RGB', 'BGR', or custom\n",
    "    normalization_params: Optional[dict] = None,\n",
    "    handle_nodata: bool = True,\n",
    "    nodata_fill: float = 0.0,\n",
    "    data_range: Optional[Tuple[float, float]] = None,\n",
    "    clip_percentiles: Optional[Tuple[float, float]] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path: Path to the .tif file\n",
    "        size: Target size for resizing (height, width)\n",
    "        bands: List of band indices to read (1-indexed)\n",
    "        band_order: Band arrangement - 'RGB', 'BGR', or 'custom'\n",
    "        normalization_params: Dict with 'mean' and 'std' for normalization\n",
    "        handle_nodata: Whether to handle nodata values\n",
    "        nodata_fill: Value to fill nodata pixels with\n",
    "        data_range: Expected data range (min, max) for scaling\n",
    "        clip_percentiles: Percentiles (low, high) for clipping extreme values\n",
    "    \n",
    "    Returns:\n",
    "        PyTorch tensor with shape (1, C, H, W)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Error Handling & Validation\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    \n",
    "    if not path.lower().endswith(('.tif', '.tiff')):\n",
    "        warnings.warn(f\"File extension suggests this might not be a TIFF file: {path}\")\n",
    "    \n",
    "    try:\n",
    "        with rasterio.open(path) as src:\n",
    "            # Validate bands\n",
    "            if max(bands) > src.count:\n",
    "                raise ValueError(f\"Requested band {max(bands)} but file only has {src.count} bands\")\n",
    "            \n",
    "            if len(bands) < 3:\n",
    "                raise ValueError(f\"Need at least 3 bands for RGB processing, got {len(bands)}\")\n",
    "            \n",
    "            # Read the specified bands\n",
    "            img = src.read(bands)  # Shape: (C, H, W)\n",
    "            \n",
    "            # Get nodata value\n",
    "            nodata_value = src.nodata\n",
    "            \n",
    "            # Get data type info\n",
    "            dtype = src.dtypes[0]\n",
    "            print(f\"Original data type: {dtype}\")\n",
    "            print(f\"Image shape: {img.shape}\")\n",
    "            print(f\"Data range: {img.min()} to {img.max()}\")\n",
    "            \n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        raise RuntimeError(f\"Failed to read rasterio file: {e}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Unexpected error reading file: {e}\")\n",
    "    \n",
    "    # Handle different data types and ranges\n",
    "    img = img.astype(np.float32)\n",
    "    \n",
    "    # Handle nodata values\n",
    "    if handle_nodata and nodata_value is not None:\n",
    "        img[img == nodata_value] = nodata_fill\n",
    "        print(f\"Handled nodata value: {nodata_value}\")\n",
    "    \n",
    "    # Handle different data ranges\n",
    "    if data_range is not None:\n",
    "        # Scale from data_range to [0, 1]\n",
    "        img = (img - data_range[0]) / (data_range[1] - data_range[0])\n",
    "        img = np.clip(img, 0, 1)\n",
    "        print(f\"Scaled data from range {data_range} to [0, 1]\")\n",
    "    elif dtype in ['uint8']:\n",
    "        # 8-bit data: scale from [0, 255] to [0, 1]\n",
    "        img = img / 255.0\n",
    "        print(\"Scaled 8-bit data to [0, 1]\")\n",
    "    elif dtype in ['uint16']:\n",
    "        # 16-bit data: scale from [0, 65535] to [0, 1]\n",
    "        img = img / 65535.0\n",
    "        print(\"Scaled 16-bit data to [0, 1]\")\n",
    "    elif img.max() > 1.0:\n",
    "        # Assume it needs scaling if max > 1\n",
    "        img = img / img.max()\n",
    "        print(f\"Scaled data by max value: {img.max()}\")\n",
    "    \n",
    "    # Clip extreme values if specified\n",
    "    if clip_percentiles is not None:\n",
    "        low_val = np.percentile(img, clip_percentiles[0])\n",
    "        high_val = np.percentile(img, clip_percentiles[1])\n",
    "        img = np.clip(img, low_val, high_val)\n",
    "        # Renormalize after clipping\n",
    "        img = (img - low_val) / (high_val - low_val)\n",
    "        print(f\"Clipped to {clip_percentiles} percentiles and renormalized\")\n",
    "    \n",
    "    # Convert to HWC format\n",
    "    img = np.transpose(img, (1, 2, 0))  # (C, H, W) -> (H, W, C)\n",
    "    \n",
    "    # Handle different band orders\n",
    "    if band_order == 'BGR' and img.shape[2] >= 3:\n",
    "        # Convert BGR to RGB\n",
    "        img = img[:, :, [2, 1, 0]]\n",
    "        print(\"Converted BGR to RGB\")\n",
    "    elif band_order == 'custom':\n",
    "        print(\"Using custom band order (no reordering applied)\")\n",
    "    \n",
    "    # Set up normalization parameters\n",
    "    if normalization_params is None:\n",
    "        # Use ImageNet statistics by default\n",
    "        normalization_params = {\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225]\n",
    "        }\n",
    "        print(\"Using ImageNet normalization parameters\")\n",
    "    else:\n",
    "        print(f\"Using custom normalization: mean={normalization_params['mean']}, std={normalization_params['std']}\")\n",
    "    \n",
    "    # Preprocessing pipeline\n",
    "    try:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=normalization_params['mean'],\n",
    "                std=normalization_params['std']\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "        print(f\"Final tensor shape: {tensor.shape}\")\n",
    "        return tensor\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error in preprocessing pipeline: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044282e6",
   "metadata": {},
   "source": [
    "### 2. Normalization Function\n",
    "\n",
    "This function provides normalization parameters specific to different types of satellite imagery. Since different satellites have varying spectral characteristics, using appropriate normalization values (mean and standard deviation) helps improve model performance and ensures consistent preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2a2384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_satellite_normalization_params(imagery_type: str = 'sentinel2') -> dict:\n",
    "    \"\"\"    \n",
    "    Args:\n",
    "        imagery_type: Type of satellite imagery ('sentinel2', 'landsat', 'imagenet')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with mean and std values\n",
    "    \"\"\"\n",
    "    if imagery_type.lower() == 'sentinel2':\n",
    "        # Sentinel-2 RGB band statistics (approximate)\n",
    "        return {\n",
    "            'mean': [0.485, 0.456, 0.406],  \n",
    "            'std': [0.229, 0.224, 0.225]\n",
    "        }\n",
    "    elif imagery_type.lower() == 'landsat':\n",
    "        # Landsat RGB band statistics (approximate)\n",
    "        return {\n",
    "            'mean': [0.5, 0.5, 0.5],  \n",
    "            'std': [0.25, 0.25, 0.25]\n",
    "        }\n",
    "    else:\n",
    "        # Default ImageNet statistics\n",
    "        return {\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2dc78",
   "metadata": {},
   "source": [
    "### Test Run Inference with DOFA (TorchGeo)\n",
    "\n",
    "This function loads a `.tif` image, preprocesses it into a tensor, and runs it through the DOFA model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7392b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_pipeline(image_path: str, imagery_type: str = 'sentinel2'):\n",
    "    try:\n",
    "        # Create appropriate normalization parameters\n",
    "        norm_params = create_satellite_normalization_params(imagery_type)\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        image_tensor = load_tif_as_tensor(\n",
    "            path=image_path,\n",
    "            size=(224, 224),\n",
    "            bands=[1, 2, 3],  # RGB bands\n",
    "            band_order='RGB',\n",
    "            normalization_params=norm_params,\n",
    "            handle_nodata=True,\n",
    "            clip_percentiles=(2, 98),  # Clip extreme 2% on each end\n",
    "            data_range=None  # Let the function auto-detect\n",
    "        )\n",
    "        \n",
    "        print(\"Image tensor shape:\", image_tensor.shape)\n",
    "        \n",
    "        # Initialize DOFA model\n",
    "        try:\n",
    "            model = DOFA(img_size=224, patch_size=16)\n",
    "            model.eval()\n",
    "            print(\"Model initialized successfully\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to initialize DOFA model: {e}\")\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Use approximate wavelengths for RGB bands (in micrometers)\n",
    "                output = model(image_tensor, wavelengths=[0.64, 0.56, 0.48])\n",
    "                print(\"Inference complete!\")\n",
    "                print(\"Output shape:\", output.shape)\n",
    "                return output\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Inference failed: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "458dce8b-d5f8-402d-bcfe-3106310cde2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data type: uint8\n",
      "Image shape: (3, 1768, 1636)\n",
      "Data range: 0 to 255\n",
      "Scaled 8-bit data to [0, 1]\n",
      "Clipped to (2, 98) percentiles and renormalized\n",
      "Using custom normalization: mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
      "Final tensor shape: torch.Size([1, 3, 224, 224])\n",
      "Image tensor shape: torch.Size([1, 3, 224, 224])\n",
      "Model initialized successfully\n",
      "Inference complete!\n",
      "Output shape: torch.Size([1, 45])\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"example.tif\"\n",
    "    try:\n",
    "        output = run_inference_pipeline(image_path, imagery_type='sentinel2')\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d1ab35",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "\n",
    "- [rasterio](https://rasterio.readthedocs.io/en/latest/)\n",
    "- [torchgeo](https://torchgeo.readthedocs.io/en/stable/)\n",
    "- [DOFA Model](https://torchgeo.readthedocs.io/en/stable/api/models.html#torchgeo.models.DOFA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53b75c-bca9-4f9e-a841-50a99bd85daa",
   "metadata": {},
   "source": [
    "### AI Disclaimer  \n",
    "Parts of this code, including debugging, error handling, and text refinement, were developed with the assistance of AI tools. AI was used to help correct code logic, fix grammar, and improve clarity in documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
